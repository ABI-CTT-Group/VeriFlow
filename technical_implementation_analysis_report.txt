VeriFlow - Technical Implementation Analysis (Merged & Updated)
================================================================
Last Updated: 2026-02-08
Hackathon Deadline: February 9, 2026 @ 5:00 PM PST
Judging Criteria: Technical Execution (40%), Innovation (30%), Impact (20%), Presentation (10%)

Status Key:
  [DONE]    = Already implemented in current codebase
  [PARTIAL] = Started but incomplete
  [TODO]    = Not yet started

Changes ordered from easiest to hardest, reflecting the CURRENT state of the codebase
(post Scholar agent rewrite, new config system, new Pydantic schemas).

================================================================================
1. Fix requirements.txt package name  [TODO]
   Effort: 1 minute | Impact: CRITICAL (app won't install without this)
================================================================================

The codebase imports `from google import genai` (new SDK) but requirements.txt still
lists `google-generativeai>=0.3.0` (the OLD, legacy package). A fresh `pip install`
will install the wrong package and the app will crash on import.

What to change:
- In backend/requirements.txt: change `google-generativeai>=0.3.0` to `google-genai>=1.0.0`
- Run `docker compose build backend` to rebuild the container

Why it matters: This is a showstopper. If judges clone the repo and run
`pip install -r requirements.txt`, the app immediately fails. It also signals
carelessness if the dependency file doesn't match the actual imports.

================================================================================
2. Update config.yaml model names to Gemini 3  [TODO]
   Effort: 5 minutes | Impact: HIGH (shows you're actually using Gemini 3)
================================================================================

Currently config.yaml maps all model aliases to "gemini-2.5-flash". For a Gemini 3
hackathon, you need to be using Gemini 3 models.

What to change:
- config.yaml: Change api_model_name values to Gemini 3 model IDs:
    gemini-2.5-pro  -> api_model_name: "gemini-3-pro-preview"
    gemini-2.0-flash -> api_model_name: "gemini-3-flash-preview"
    gemini-2.0-thinking -> api_model_name: "gemini-3-pro-preview" (with thinking)
- Also update GeminiClient default: self.model_name = "gemini-3-flash-preview"
- Update SPEC.md and PLAN.md model references if not already done

Why it matters: The hackathon is literally called "Gemini 3". Using Gemini 2.5
models is like entering a React hackathon with jQuery. Judges will check your
API calls/config to verify you're using the right models.

================================================================================
3. Fix temperature to 1.0 for Gemini 3  [PARTIAL]
   Effort: 5 minutes | Impact: MEDIUM
================================================================================

The Gemini 3 docs explicitly warn: "Changing the temperature (setting it below 1.0)
may lead to unexpected behavior, such as looping or degraded performance."

Current state: GeminiClient.analyze_file() defaults to temperature=1.0 (good), but
config.yaml sets temperature: 0.2 for all models, and scholar.py passes
`temperature=model_params.get("temperature", 1.0)` which will pick up 0.2 from config.

What to change:
- config.yaml: Change temperature from 0.2 to 1.0 for all model entries
- Or remove temperature from config entirely (let it default to 1.0)
- Rely on response_schema + thinking_level for output quality instead of low temperature

Why it matters: Shows judges you read the Gemini 3 documentation. With structured
output (response_schema) and thinking_level control, you don't need low temperature
as a reliability crutch.

================================================================================
4. Fix tests to match current GeminiClient API  [TODO]
   Effort: 30-60 minutes | Impact: HIGH (code quality signal)
================================================================================

The test suite is completely broken. Tests still reference the OLD API surface:
- test_gemini_client.py mocks `genai.configure`, `client.model`, `generate_json()`
- conftest.py creates fixtures for `generate_json`, `generate_content`
- None of these methods exist in the current GeminiClient (which only has `analyze_file()`)

What to change:
- Rewrite test_gemini_client.py to test `analyze_file()` with mocked `client.files.upload()`
  and `client.models.generate_content()`
- Update conftest.py fixtures to mock the new SDK classes
- Add tests for PromptManager and AppConfig
- Add at least one FastAPI TestClient test for the /publications/upload endpoint
- Aim for at least 30-40% coverage on critical paths

Why it matters: The criteria asks "Is the code of good quality?" Tests that don't
even run are worse than no tests at all -- they show the codebase is in a broken state.

================================================================================
5. Add generate_text() method to GeminiClient  [TODO]
   Effort: 30 minutes | Impact: HIGH (unblocks Engineer/Reviewer migration)
================================================================================

The current GeminiClient only has analyze_file() which uploads a PDF and uses
response_schema=AnalysisResult. The Engineer and Reviewer agents don't process PDFs --
they take JSON/text input and produce different structured outputs. They need a
general-purpose text generation method.

What to change:
- Add a `generate_text(prompt, system_instruction, response_schema=None, temperature=1.0)`
  method that calls `client.models.generate_content()` without file upload
- Accept an optional response_schema parameter for structured output
- This becomes the foundation for Engineer and Reviewer agents

Why it matters: Without this, only the Scholar agent works. 2 out of 3 agents being
broken means the app is fundamentally non-functional.

================================================================================
6. Add thinking_level parameter to agent calls  [TODO]
   Effort: 1 hour | Impact: HIGH (flagship Gemini 3 feature)
================================================================================

This is the marquee Gemini 3 feature. The thinking_level parameter controls how
deeply the model reasons before responding. You're currently not using it at all.

What to change:
- Add thinking_level parameter to GeminiClient methods (analyze_file, generate_text)
- Pass it via GenerateContentConfig:
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(thinking_level="HIGH")
    )
- Scholar Agent: thinking_level="HIGH" (complex scientific extraction)
- Engineer Agent: thinking_level="HIGH" (CWL generation is complex)
- Reviewer Agent: thinking_level="MEDIUM" for validation, "LOW" for error translation
- Add thinking_level to config.yaml per-agent settings
- Note: The existing `thought_process` field in AnalysisResult aligns well -- Gemini 3's
  thinking output can populate it naturally

Why it matters: Not using thinking_level is like building a GPT-4 app that only uses
GPT-3.5 features. It directly addresses "Does the project leverage Gemini 3?"

================================================================================
7. Migrate Engineer Agent to new SDK  [TODO]
   Effort: 2-3 hours | Impact: HIGH (functional completeness)
================================================================================

The Engineer Agent is completely broken. It still imports the deleted
`get_gemini_client()` function and calls `self.gemini.generate_json()` which doesn't
exist. It's commented out in agents/__init__.py.

What to change:
- Rewrite engineer.py to use the new pattern (matching scholar.py):
  - Import GeminiClient directly
  - Load config from AppConfig, prompts from PromptManager
  - Use generate_text() (from change #5) with appropriate response_schema
- Create a Pydantic schema for Engineer output (WorkflowResult with CWL, Dockerfiles,
  graph nodes/edges) in schemas.py
- Re-enable in agents/__init__.py
- Ensure the /workflows/assemble endpoint works end-to-end

Why it matters: The workflow generation pipeline is a core feature. Without the
Engineer agent, VeriFlow can extract information from papers but can't do anything
with it.

================================================================================
8. Migrate Reviewer Agent to new SDK  [TODO]
   Effort: 2-3 hours | Impact: HIGH (functional completeness)
================================================================================

Same situation as Engineer -- broken imports, deleted methods, commented out.

What to change:
- Rewrite reviewer.py to use the new pattern
- Create a Pydantic schema for Reviewer output (ValidationResult with issues,
  suggestions, severity levels) in schemas.py
- Re-enable in agents/__init__.py
- Wire up the validation endpoint

Why it matters: The validation pipeline is what makes VeriFlow more than just an
extraction tool. The Scholar -> Engineer -> Reviewer pipeline is the core value
proposition.

================================================================================
9. Pydantic response_schema for all agents  [PARTIAL]
   Effort: 2 hours | Impact: HIGH (proper Gemini 3 API usage + code quality)
================================================================================

The Scholar agent already uses response_schema=AnalysisResult for guaranteed
structured output. The Engineer and Reviewer agents need the same treatment.

Current state:
- [DONE] Scholar: Uses response_schema=AnalysisResult via analyze_file()
- [TODO] Engineer: Needs WorkflowResult schema (CWL, Dockerfiles, graph)
- [TODO] Reviewer: Needs ValidationResult schema (issues, suggestions)

What to change:
- Define WorkflowResult in schemas.py (matching the JSON structure in prompts.yaml
  engineer_workflow section)
- Define ValidationResult in schemas.py (matching reviewer_translate output)
- Pass these as response_schema in the respective agent calls
- Remove any remaining "Respond ONLY with valid JSON" prompt hacks

Why it matters: Native structured output is deterministic, never fails to parse,
and is a key Gemini 3 feature. Using it for all agents (not just Scholar) shows
consistent, proper API usage.

================================================================================
10. Implement Thought Signatures for multi-turn conversations  [TODO]
    Effort: 2-3 hours | Impact: MEDIUM
================================================================================

Gemini 3 introduces thought signatures that must be preserved across multi-turn
conversations. This is critical for function calling and maintaining reasoning chains.

What to change:
- Store and pass back thought_signatures from Gemini responses in conversation history
- Add multi-turn support to GeminiClient (or use client.chats.create())
- Use this for the Reviewer agent's iterative validation flow
  (validate -> suggest fix -> re-validate)
- This naturally builds on thinking_level (change #6)

Why it matters: This is a Gemini 3-specific feature that demonstrates you understand
the new model's architecture. Especially relevant for the Reviewer's iterative flow.

================================================================================
11. Grounding with Google Search for the Scholar Agent  [TODO]
    Effort: 4-6 hours | Impact: HIGH (verification + Gemini 3 feature showcase)
================================================================================

Gemini 3 supports Grounding with Google Search combined with structured outputs.
Your Scholar Agent identifies tools, models, and measurements but can't verify them
against real-world sources.

What to change:
- Enable google_search tool in Scholar agent's tool configuration:
    tools=[types.Tool(google_search=types.GoogleSearch())]
- Use grounding to verify/enrich identified_tools (validate GitHub URLs, find docs)
- Use grounding to validate identified_models (check architecture claims, find model cards)
- Display grounding sources/citations in the frontend confidence panel
- Add grounding_metadata to the AnalysisResult schema

Why it matters: Grounding lets the Scholar verify tool URLs, find documentation links,
check model architectures against HuggingFace/Papers with Code. It turns
confident-sounding hallucinations into verifiable facts -- perfectly aligned with
VeriFlow's mission of research reliability. This is a powerful Gemini 3 feature.

================================================================================
12. Remove mocks and make the app functional end-to-end  [TODO]
    Effort: 4-6 hours | Impact: CRITICAL (judges will test the demo)
================================================================================

Several parts of the app fall back to mock data:
- publications.py returns hardcoded mock ISA hierarchy when no cache entry exists
- executions.py runs _run_mock_execution() with asyncio.sleep simulations
- executions.py returns mock result files
- engineer.py has _generate_fallback_workflow() that creates a fake graph
- The property update endpoint (PUT /study-design/nodes/{id}/properties) is a no-op

What to change:
- Remove mock data fallbacks (return proper errors instead)
- Ensure the full pipeline works: Upload PDF -> Scholar extracts ISA -> Engineer
  generates CWL workflow -> Reviewer validates -> Results displayed
- At minimum, the MAMA-MIA example should work end-to-end with real Gemini 3 calls
- Test the frontend flow manually: Load paper -> View extraction -> View workflow

Why it matters: The criteria explicitly asks "Is the code functional?" Mock execution
with asyncio.sleep is not functional code. If judges trigger the demo flow and see
canned responses, that's a major scoring hit.

================================================================================
13. Agentic Vision for PDF analysis (Gemini 3 Flash)  [TODO]
    Effort: 6-8 hours | Impact: VERY HIGH (Innovation + Gemini 3 showcase)
================================================================================

Currently the Scholar agent sends the PDF as a file upload. Gemini 3 Flash's
Agentic Vision can analyze images with a think-act-observe loop, which is perfect
for understanding methodology diagrams and flowcharts in scientific papers.

What to change:
- Use Gemini 3 Flash's agentic vision to analyze architecture diagrams
- Have the model identify pipeline steps from figures (not just text)
- Cross-reference visual analysis with text-based extraction
- Show in the frontend which figure informed which workflow step

Why it matters: Scientific papers communicate critical methodology through figures.
Extracting workflow structure from figures is far more impressive than text-only
extraction. This addresses both "Does the project leverage Gemini 3?" and
"Innovation/Wow Factor" (30%).

================================================================================
14. Implement real agentic tool-use loops  [TODO]
    Effort: 8-10 hours | Impact: VERY HIGH (what Gemini 3 was built for)
================================================================================

Your current agents are single-shot: one prompt in, one response out. This doesn't
demonstrate the agentic capabilities Gemini 3 was designed for. A real agentic flow
would have agents that use tools, observe results, and iterate.

What to change:
- Implement function calling with defined tools:
  - validate_cwl: Engineer calls this to check generated CWL syntax
  - search_pubmed: Scholar verifies paper references
  - check_docker_image: Engineer verifies Docker images exist
  - lookup_tool_docs: Scholar finds documentation for identified tools
- Engineer Agent: generate CWL -> validate -> observe errors -> fix -> repeat
- Reviewer Agent: validate -> identify issues -> attempt auto-fix -> re-validate
- This naturally uses thought signatures (change #10) across multi-turn tool-use

Why it matters: Gemini 3 was explicitly designed for agentic workflows. Single-shot
prompting is a Gemini 1.5-era pattern. Tool-use with iterative refinement shows
judges you're building a genuinely Gemini 3-native application.

================================================================================
QUICK-WIN PRIORITY ORDER (for the remaining ~24 hours before deadline)
================================================================================

Phase 1 - "Make it not broken" (2 hours):
  #1  Fix requirements.txt                    [1 min]
  #2  Update config.yaml to Gemini 3 models   [5 min]
  #3  Fix temperature to 1.0                  [5 min]
  #5  Add generate_text() to GeminiClient     [30 min]
  #4  Fix broken tests                        [30-60 min]

Phase 2 - "Make it complete" (5 hours):
  #7  Migrate Engineer Agent                  [2-3 hr]
  #8  Migrate Reviewer Agent                  [2-3 hr]

Phase 3 - "Show off Gemini 3" (3 hours):
  #6  Add thinking_level parameter            [1 hr]
  #9  Pydantic response_schema for all agents [2 hr]

Phase 4 - "Impress the judges" (8+ hours):
  #11 Grounding with Google Search            [4-6 hr]
  #12 End-to-end demo flow (remove mocks)     [4-6 hr]
  #10 Thought signatures for multi-turn       [2-3 hr]

Phase 5 - "Win the hackathon" (14+ hours):
  #13 Agentic Vision for PDF figures          [6-8 hr]
  #14 Real agentic tool-use loops             [8-10 hr]

================================================================================
SUMMARY TABLE
================================================================================
 #  | Change                              | Status  | Effort   | Impact
----|-------------------------------------|---------|----------|--------
 1  | Fix requirements.txt                | TODO    | 1 min    | CRITICAL
 2  | Config.yaml -> Gemini 3 models      | TODO    | 5 min    | HIGH
 3  | Fix temperature to 1.0              | PARTIAL | 5 min    | MEDIUM
 4  | Fix broken tests                    | TODO    | 30-60min | HIGH
 5  | Add generate_text() to GeminiClient | TODO    | 30 min   | HIGH
 6  | Add thinking_level parameter        | TODO    | 1 hr     | HIGH
 7  | Migrate Engineer Agent              | TODO    | 2-3 hr   | HIGH
 8  | Migrate Reviewer Agent              | TODO    | 2-3 hr   | HIGH
 9  | Pydantic schemas for all agents     | PARTIAL | 2 hr     | HIGH
10  | Thought signatures (multi-turn)     | TODO    | 2-3 hr   | MEDIUM
11  | Grounding with Google Search        | TODO    | 4-6 hr   | HIGH
12  | End-to-end demo (remove mocks)      | TODO    | 4-6 hr   | CRITICAL
13  | Agentic Vision for PDF figures      | TODO    | 6-8 hr   | VERY HIGH
14  | Real agentic tool-use loops         | TODO    | 8-10 hr  | VERY HIGH

================================================================================
WHAT CHANGED SINCE ORIGINAL ANALYSIS
================================================================================

Items COMPLETED or PARTIALLY completed since original analysis:
- [DONE] Scholar Agent uses response_schema=AnalysisResult (Pydantic structured output)
- [DONE] New config system (config.yaml + AppConfig singleton)
- [DONE] New prompt management (prompts.yaml + PromptManager singleton)
- [DONE] Scholar Agent fully migrated to google-genai SDK
- [DONE] GeminiClient uses native PDF upload (client.files.upload())
- [PARTIAL] Temperature defaults to 1.0 in GeminiClient (but config overrides to 0.2)

Items ADDED (new problems discovered):
- requirements.txt has wrong package name (was correct before, got reverted)
- config.yaml uses Gemini 2.5 model names (not Gemini 3)
- Tests are completely broken (test old API that no longer exists)
- GeminiClient missing generate_text() method (needed for non-PDF agents)
- Engineer and Reviewer agents are broken and disabled

Items UNCHANGED from original analysis:
- thinking_level not implemented
- Thought signatures not implemented
- Grounding not implemented
- Mocks still present in API endpoints
- Agentic Vision not implemented
- Real agentic loops not implemented

Sources:
- https://ai.google.dev/gemini-api/docs/gemini-3
- https://gemini3.devpost.com/
- https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/
- https://blog.google/technology/developers/gemini-3-developers/
