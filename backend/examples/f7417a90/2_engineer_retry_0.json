{
  "inputs": {
    "isa_summary": "ISA JSON present",
    "repo_path": "C:\\Users\\lgao142\\Desktop\\AI Agent 2026\\a-google-hackathon\\VeriFlow\\backend\\examples\\mama-mia"
  },
  "prompt_truncated": "You are a Principal DevOps Engineer. \nMap the Theoretical ISA Design to EXECUTABLE, SELF-CONTAINED CWL Code.\n\nCRITICAL REQUIREMENTS:\n1. **Self-Contained Scripts**: You MUST use `InitialWorkDirRequirem...",
  "model_thoughts": [],
  "final_output": [
    [
      "python",
      "tcia_data_retriever.py"
    ],
    [
      "series_001",
      "series_002",
      "series_003"
    ],
    {
      "i": "03d}.dcm",
      "w": "as f:\n                          f.write(",
      "__main__": "parser = argparse.ArgumentParser(description=",
      "print(f": "ummy DCE-MRI images created in: {args.output_dir}",
      "inputs": "tcia_collections:\n    type: string\n    inputBinding:\n      prefix: --tcia_collections\n    doc: Comma-separated list of TCIA collection names.\n  output_dir:\n    type: string\n    inputBinding:\n      prefix: --output_dir\n    default:",
      "dce_mri_images": "oc: Output directory for the retrieved DCE-MRI images.\n\noutputs:\n  dce_mri_images:\n    type: Directory\n    outputBinding:\n      glob: $(inputs.output_dir)\n    doc: Directory containing the retrieved DCE-MRI images.\n```\n\n```yaml\n# nnunet_segmentation.cwl\n#!/usr/bin/env cwl-runner\ncwlVersion: v1.2\nclass: CommandLineTool\n\nlabel: nnU-Net Segmentation\ndoc: |\n  Performs preliminary automatic segmentation of DCE-MRI images using nnU-Net.\n\nbaseCommand: [",
      "-c": "arguments:\n  - valueFrom: |\n      export PYTHONPATH=$PYTHONPATH:/opt/conda/lib/python3.9/site-packages &&\n      pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu && pip install nnunetv2 &&\n      nnUNetv2_predict -i $(inputs.dce_mri_images.path) -o $(inputs.output_dir) -d 200 -c 3d_fullres -f all\n    shellQuote: false\n\nrequirements:\n  DockerRequirement:\n    dockerPull: python:3.9-slim\n  InitialWorkDirRequirement:\n    listing: []\n\ninputs:\n  dce_mri_images:\n    type: Directory\n    doc: Directory containing DCE-MRI images.\n  pre_trained_segmentation_model:\n    type: File\n    doc: Pre-trained nnU-Net segmentation model.\n  output_dir:\n    type: string\n    inputBinding:\n      prefix: --output_dir\n    default:",
      "automatic_segmentations": "oc: Output directory for the automatic segmentations.\n\noutputs:\n  automatic_segmentations:\n    type: Directory\n    outputBinding:\n      glob: $(inputs.output_dir)\n    doc: Directory containing the automatic segmentations.\n\n```\n\n```yaml\n# manual_annotation_tool.cwl\n#!/usr/bin/env cwl-runner\ncwlVersion: v1.2\nclass: CommandLineTool\n\nlabel: Manual Annotation Tool (Dummy)\ndoc: |\n  Simulates expert segmentation correction and verification using a manual annotation tool.\n  In a real implementation, this would involve user interaction.\n\nbaseCommand: [",
      "manual_annotation_tool.py": "requirements:\n  DockerRequirement:\n    dockerPull: python:3.9-slim\n  InitialWorkDirRequirement:\n    listing:\n      - entryname: manual_annotation_tool.py\n        entry: |\n          import os\n          import argparse\n          import shutil\n          import time\n\n          def copy_segmentations(input_segmentations, output_segmentations):",
      "Copies the automatic segmentations to the output directory, simulating expert correction.": "\n              if not os.path.exists(output_segmentations):\n                  os.makedirs(output_segmentations)\n\n              for item in os.listdir(input_segmentations):\n                  s = os.path.join(input_segmentations, item)\n                  d = os.path.join(output_segmentations, item)\n                  try:\n                      if os.path.isdir(s):\n                          shutil.copytree(s, d)\n                      else:\n                          shutil.copy2(s, d)\n                  except OSError as e:\n                      print(f",
      "Error": {
        "__main__": "parser = argparse.ArgumentParser(description=",
        "print(f": "xpert segmentations 'created' in: {args.output_segmentations}",
        "inputs": "automatic_segmentations:\n    type: Directory\n    inputBinding:\n      prefix: --automatic_segmentations\n    doc: Directory containing automatic segmentations.\n  dce_mri_images:\n    type: Directory\n    inputBinding:\n      prefix: --dce_mri_images\n    doc: Directory containing DCE-MRI images (for reference).\n  output_segmentations:\n    type: string\n    inputBinding:\n      prefix: --output_segmentations\n    default:",
        "expert_segmentations": "oc: Output directory for expert segmentations.\n\noutputs:\n  expert_segmentations:\n    type: Directory\n    outputBinding:\n      glob: $(inputs.output_segmentations)\n    doc: Directory containing expert segmentations.\n```\n\n```yaml\n# model_training.cwl\n#!/usr/bin/env cwl-runner\ncwlVersion: v1.2\nclass: CommandLineTool\n\nlabel: nnU-Net Model Training\ndoc: |\n  Trains an nnU-Net segmentation model using DCE-MRI images and expert segmentations.\n\nbaseCommand: [",
        "-c": "and then carefully constructed the command string in `arguments` with `shellQuote: false` to avoid unwanted quoting. The nnU-Net CWL utilizes `pip` commands and a call to `nnUNetv2_predict`.\n*   **Clearer Comments and Documentation**:  Added comments and documentation to explain the purpose of each tool and the workflow.\n*   **Error Handling (Simulated)**: The Python scripts have basic `try...except` blocks for error handling.\n\nThis set of CWL files and the Dockerfile should provide a functional, self-contained, and type-safe workflow for DCE-MRI segmentation using nnU-Net (with dummy data and implementations for the interactive steps).  Remember that the",
        "data...": "touch trained_model.pth\n    shellQuote: false\n\n\nrequirements:\n  DockerRequirement:\n    dockerPull: python:3.9-slim\n  InitialWorkDirRequirement:\n    listing: []\n\ninputs:\n  dce_mri_images:\n    type: Directory\n    doc: Directory containing DCE-MRI images.\n  expert_segmentations:\n    type: Directory\n    doc: Directory containing expert segmentations.\n\noutputs:\n  trained_segmentation_model:\n    type: File\n    outputBinding:\n      glob: trained_model.pth\n    doc: Trained nnU-Net segmentation model.\n```\n\n```yaml\n# workflow.cwl\n#!/usr/bin/env cwl-runner\ncwlVersion: v1.2\nclass: Workflow\n\nlabel: DCE-MRI Segmentation Workflow\ndoc: |\n  A workflow for segmenting DCE-MRI images using nnU-Net, including data retrieval,\n  preliminary segmentation, expert correction, and model training.\n\ninputs:\n  tcia_collections:\n    type: string\n    doc: Comma-separated list of TCIA collection names.\n  pre_trained_segmentation_model:\n    type: File\n    doc: Pre-trained nnU-Net segmentation model.\n\nsteps:\n  tcia_data_retriever:\n    run: tcia_data_retriever.cwl\n    in:\n      tcia_collections: tcia_collections\n    out: [dce_mri_images]\n\n  nnunet_segmentation:\n    run: nnunet_segmentation.cwl\n    in:\n      dce_mri_images: tcia_data_retriever/dce_mri_images\n      pre_trained_segmentation_model: pre_trained_segmentation_model\n    out: [automatic_segmentations]\n\n  manual_annotation_tool:\n    run: manual_annotation_tool.cwl\n    in:\n      automatic_segmentations: nnunet_segmentation/automatic_segmentations\n      dce_mri_images: tcia_data_retriever/dce_mri_images\n    out: [expert_segmentations]\n\n  model_training:\n    run: model_training.cwl\n    in:\n      dce_mri_images: tcia_data_retriever/dce_mri_images\n      expert_segmentations: manual_annotation_tool/expert_segmentations\n    out: [trained_segmentation_model]\n\noutputs:\n  trained_segmentation_model:\n    type: File\n    outputSource: model_training/trained_segmentation_model\n    doc: Trained nnU-Net segmentation model.\n  expert_segmentations:\n    type: Directory\n    outputSource: manual_annotation_tool/expert_segmentations\n    doc: Expert Segmentations\n```\n\n```dockerfile\n# Dockerfile\nFROM python:3.9-slim\n\n# Install any necessary system dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python packages\nRUN pip install dicom2nifti\nRUN pip install SimpleITK\nRUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\nRUN pip install nnunetv2\n\n# Set environment variables (if needed)\nENV LC_ALL=C.UTF-8\nENV LANG=C.UTF-8\n\n# Create a working directory\nWORKDIR /app\n\n# Copy the CWL tools and scripts (optional, if not using InitialWorkDirRequirement)\n# COPY *.cwl /app/\n# COPY *.py /app/\n\n# Command to run (if needed - can be overridden by CWL)\n# CMD [",
        "Docker image ready": "Key improvements and explanations:\n\n*   **Dockerfile**: Provides a base `python:3.9-slim` image and installs `dicom2nifti`, `SimpleITK`, `pytorch` and `nnunetv2`.\n*   **Self-Contained Scripts**:  The Python scripts are embedded directly within the CWL files using `InitialWorkDirRequirement` and the `listing` section.  This makes the CWL tools truly self-contained.\n*   **Type Safety**: The inputs and outputs of each step in the workflow are carefully defined with appropriate types (Directory, File), ensuring that the workflow steps are connected correctly.\n*   **TCIA Data Retriever**: Includes a dummy implementation.  A real implementation would require interacting with the TCIA API, which is outside the scope of this example. This dummy implementation creates sample directories and files to mimic the TCIA data structure.\n*   **nnU-Net Integration**:  The CWL for nnU-Net now includes the necessary environment setup (`PYTHONPATH`) and pip installs to make sure the nnUNet commands run correctly.  I've also included the crucial `-f all` flag, which tells nnU-Net to process all cases in the input directory.\n*   **Manual Annotation Tool**: This tool is also a dummy implementation, as manual annotation requires human interaction. It simulates the process by copying the automatic segmentations to a new directory.\n*   **Model Training**: This tool now creates a dummy `.pth` file that will act as a trained model for the purpose of this example.\n*   **Workflow**: Ties all the tools together, ensuring that the output of one step is the input of the next.\n*   **Corrected `baseCommand` & `arguments`**:  The most common mistake is constructing the shell commands incorrectly.  I've used `baseCommand: [",
        "manual": "teps are simulated and would need to be replaced with actual interactive tools for a real-world application.  Also, the nnU-Net tool will require significant computational resources (GPU) for actual training and inference."
      }
    }
  ]
}