The Proposal:

Project Title: VeriFlow (The Verifiable Workflow Architect)

Elevator Pitch

VeriFlow is an autonomous "Research Reliability Engineer" designed primarily to ensure Scientific Reproducibility. It solves the crisis of non-replicable science by using Gemini 3 to ingest scientific publications, extract their methodological logic, and verify their code repositories. It automatically packages these components into verifiable, executable "Lego blocks" (Docker containers described in CWL) and assembles them into a standardized, FAIR-compliant workflow. By leveraging the SPARC Dataset Structure (SDS) as its internal currency and the ISA Framework for experimental context, VeriFlow ensures that every measurement, tool, model, and study design is interoperable. For the hackathon, VeriFlow focuses on Autonomous Replication: allowing researchers to chat with the system to visualize and instantiate a verified workflow from a PDF and execute it on the paper's original data. Future phases will extend this to dynamically add new capabilities (e.g., calculating the volume of a tumor from the segmentation masks generated by the replicated workflow) and run workflows on user-provided datasets.

1\. Data Object Standardization Strategy

To ensure VeriFlow is future-proof and interoperable, we strictly separate the Methodology (how agents reason) from the Serialization Rules (how data is stored). This allows us to swap standards (e.g., moving to RO-Crates in the future) without retraining the agents.

The "Universal Adapter" Infrastructure

 \* Core Standard (Hackathon): We utilize the SPARC Dataset Structure (SDS) (https://docs.sparc.science/docs/sparc-dataset-structure) as the fundamental format for all internal data objects.

 \* Internal Description Standard: We use Common Workflow Language (CWL) as the internal description format for all Tool and Workflow Data Objects. This maximizes interoperability, allowing the same workflow definition to be run by different execution engines.

 \* Execution Engine (Hackathon): For the purposes of this hackathon, we utilize Apache Airflow 3 as the execution engine. We leverage its API to execute and monitor workflow execution in VeriFlow. Any necessary conversion from the internal CWL description to the Airflow execution format is performed dynamically at runtime.

 \* ISA Framework Integration: We leverage the ISA (Investigation, Study, Assay) framework to extract and serialize the experimental context from papers.

 \* Serialization Tooling: We employ sparc-me (https://github.com/SPARC-FAIR-Codeathon/sparc-me) to programmatically generate these datasets.

 \* Adapter Pattern: The infrastructure is designed with adapters. While SDS is the internal standard for metadata and organization, "Input Adapters" convert these SDS objects into the specific file formats required by individual computational tools. VeriFlow dynamically adds adapter tools as needed to help match Measurement Samples and inputs to Tools (e.g., resampling, file conversion). If a match is not feasible or appropriate, the system will propose an alternative approach. In the future, "Output Adapters" could serialize the same objects into RO-Crates or BIDS.

 \* VeriFlow API: The tool exposes a comprehensive REST/gRPC API, enabling programmatic access to all functions—from parsing PDFs and generating SDS objects to executing workflows and querying the Knowledge Base—allowing external scripts to drive the entire replication lifecycle.

Data Object Rules (The "SDS Blueprints")

 \* Separation of Concerns: We enforce strict separation of data objects. Tools, Models, Investigations, Studies, and Assays are NOT bundled inside measurement datasets. Instead, specific SDS datasets are created for each entity type. This ensures they remain modular, reusable objects independent of the specific data they process.

 \* ISA Hierarchy Serialization:

   \* Investigation, Study, and Assay objects are extracted and serialized as separate SDS datasets.

   \* Their descriptions are stored in the primary folder of their respective datasets.

   \* Unlike measurement datasets, these ISA datasets do not contain subjects or samples.

   \* Linking: Metadata must explicitly link these objects together (Assay \\rightarrow Study \\rightarrow Investigation). Each Assay will have one Workflow associated with it and link to the specific set of Measurements (samples) it processes, which may come from one or more external Measurement SDS datasets.

 \* Models vs. Tools:

   \* Models (e.g., pre-trained weights for nnU-Net) are distinguished from Tools (the executable software, e.g., the nnU-Net code).

   \* Models are serialized in their own dedicated Model SDS.

   \* A model dataset is similar to a measurement dataset except that it doesn’t have subjects or samples, with the model being stored in the primary folder.

   \* Linking: The Model SDS metadata must explicitly link to the Tool SDS that it is associated with.

 \* Derived Data & Provenance:

   \* When a tool generates new data (e.g., a tumor segmentation mask), this derived data is placed in the primary folder of a newly created Measurement SDS.

   \* Provenance: The metadata of this new dataset must explicitly link back to:

     \* The Original Source SDS (e.g., the MRI data) via wasDerivedFrom.

     \* The specific Tool SDS and Workflow SDS used to generate it, ensuring full reproducibility.

 \* Workflow Inputs & Outputs:

   \* A Workflow is defined by Inputs and Outputs that map to the underlying tools.

   \* Execution Logic: Running the workflow involves running each tool for each subject in the measurement dataset.

   \* Intermediate Data: By default, only the final Workflow Outputs are saved as persistent SDS samples. Intermediate outputs (passed between internal tools) are transient. However, the system provides a user option to "Save Intermediate Results," which forces the serialization of these transient steps into SDS datasets.

   \* Routing: Workflow outputs can be routed to the same SDS if appropriate, but the user can specify if specific samples (e.g., a critical biomarker) should be routed to a different, dedicated SDS.

 \* Workflows as Objects: The orchestration logic (CWL) is itself stored as a distinct Data Object within its own SDS. This CWL definition acts as the "source of truth" to maximize interoperability. When execution is required, this CWL object is converted (if necessary) to the format required by the execution engine (e.g., Airflow 3).

 \* Metadata Enrichment & Type Mapping:

   \* All objects must be populated with rich metadata tags (using the SPARC ontology) to enable automated execution.

   \* MIME-Types & File Formats: To enable automated tool assembly and input/output matching, we mandate the use of the additional\_type field in the SDS manifest files.

   \* Implementation: Each file added to an SDS must include a specific MIME-type or standard file format identifier (e.g., edam:format\_3987 for NIfTI) in its additional\_type metadata. This field can also include software version numbers if needed.

   \* Usage: These descriptors are used by the Engineer Agent to strictly enforce compatibility between tool requirements (e.g., "Input requires application/x-nifti") and available measurement data, ensuring that workflows are assembled with type safety.

 \* Incremental Population: The SDS datasets are populated throughout the stages. For example, during the Workflow Description Templating stage, the assay files and their metadata are added to the Assay SDS, while only the measurement metadata is added to the Measurement SDS (as the data would only be accessed during the Workflow Execution and Diagnostics stage).

 \* Exportability: At each stage of VeriFlow, it is expected that the user can download the SDS datasets.

2\. Methodology: The "Triad of Trust"

VeriFlow replaces generic AI agents with three specialized roles powered by Gemini 3\. These agents follow the standardization rules defined above to ensure consistency across the four defined stages: Workflow Description Templating, Workflow Execution Templating, Workflow Execution and Diagnostics, and Workflow Results Review.

Agent A: The Scholar (Literature & Metadata)

 \* Role: Extracts the "Theory" and populates Data Objects.

 \* Tech: Gemini 3 (Multimodal & Long Context).

 \* Stage: Primarily active during Workflow Description Templating.

 \* Task:

   \* Ingestion: Scans the PDF text, equations, and system diagrams to understand the sequence of operations.

   \* ISA & Object Creation: Responsible for creating the distinct SDS datasets. It utilizes sparc-me logic to define separate datasets for:

     \* Investigation, Study, Assay (Context).

     \* Measurements (Subject/Sample raw data).

     \* Tools (Executable code).

     \* Models (Pre-trained weights).

   \* Metadata Population: Extracts entities (e.g., "T1-weighted MRI", "nnU-Net Weights") and populates the SDS metadata fields according to the "SDS Blueprints," ensuring the Assay links to the correct Workflow and Measurements, and the Model links to the correct Tool. Crucially, it populates Tool and Workflow objects in CWL format to serve as the internal description standard.

Agent B: The Engineer (Code & Infrastructure)

 \* Role: Handles the "Reality" (messy code).

 \* Tech: Gemini 3 (Code Optimization).

 \* Stage: Active during Workflow Execution Templating and Workflow Execution and Diagnostics.

 \* Task:

   \* Dependency Forensics: Scans GitHub repositories. If requirements.txt is missing, it infers libraries and versions based on code timestamps and import statements.

   \* Containerization: Generates a verifiable Dockerfile and wraps it in CWL. These CWL definitions are stored in their own dedicated Workflow SDS.

   \* Execution Conversion: At runtime, it performs any necessary conversion of the CWL workflow definition to the execution engine's format (e.g., generating Airflow 3 DAGs from the CWL source).

   \* Adapter Generation: Writes the specific "glue code" needed to feed data from a Measurement SDS and weights from a Model SDS into the containerized tool. It dynamically adds adapter tools (e.g., image re-slicing, format conversion) to the workflow to ensure measurement samples match tool inputs. It leverages the additional\_type (MIME-type) metadata to identify mismatches (e.g., connecting a DICOM output to a NIfTI input) and injects the appropriate converters.

   \* Self-Healing: If a match is not feasible, it flags the incompatibility to the Reviewer Agent to propose an alternative.

Agent C: The Reviewer (Validation, Support & Interface)

 \* Role: The "Senior Developer" Interface & Full-Cycle Support.

 \* Tech: Gemini 3 (Deep Think).

 \* Stage: Active throughout, with focus on Workflow Results Review.

 \* Task:

   \* Services Integration: VeriFlow and its agents can leverage two services:

     \* a. Catalogue: A list of all the different data objects that have been extracted from the paper.

     \* b. Dataset Viewer: A service where the user can:

       \* i. Directly (or conversationally through the reviewer agent) dynamically edit metadata of each dataset.

       \* ii. Navigate and view files using embedded viewers e.g., view mesh or segmentation using VolView.

   \* Proactive Data Validation (Creation Phase): Acts as a gatekeeper, validating that the benchmark data retrieved from the paper matches the generated SDS schemas before execution begins. For example, it checks the additional\_type metadata to ensure the downloaded sample data is in the correct 3D NIfTI format required by the tool.

   \* Runtime Diagnostics (Execution Phase): Monitors the workflow live via the Airflow 3 API. If a tool crashes during replication, the Reviewer Agent doesn't just show an error code; it translates the technical Docker/Airflow logs into actionable scientific advice.

   \* Scientific "Unit Tests" (Diagnosis Phase): Validates input/output compatibility by checking the SDS metadata. For example, it confirms that a Sample tagged with unit: voxel is not fed into a Tool expecting unit: mm3.

3\. Case Studies (Applied Examples)

To demonstrate VeriFlow's versatility, we apply the methodology above to two distinct datasets and workflows. For the current hackathon phase, the focus is on Replicating the results using the data provided by the papers.

Example A: Breast Cancer Segmentation (The "MAMA-MIA" Workflow)

 \* Source: A large-scale multicenter breast cancer DCE-MRI benchmark dataset.

 \* The Goal (Replication): Replicate the automated tumor segmentation described in the paper using the paper's own benchmark dataset.

 \* VeriFlow Execution:

   \* Stage 1: Workflow Description Templating: Scholar Agent ingests the PDF. Identifies the Investigation (Breast Cancer Benchmark) and Study (Multicenter validation). It creates an Assay SDS for the segmentation task. It creates an Input Measurement SDS for the MRI scans (populating only metadata). It distinguishes the nnU-Net code (Tool SDS) from the paper's provided pre-trained weights (Model SDS), describing both in CWL format.

   \* Stage 2: Workflow Execution Templating: Reviewer Agent checks the downloaded benchmark data. Engineer Agent clones the repo and builds the container. It dynamically adds a NIfTI-to-NIfTI resampling adapter to match the input resolution. It configures the output logic so that the resulting segmentation mask is stored as defined by the SDS Blueprints. It ensures the Tool container correctly mounts the weights from the Model SDS.

   \* Stage 3: Workflow Execution and Diagnostics: It converts the final CWL definition into an Airflow 3 DAG. The system runs the workflow, iterating each tool for each subject in the measurement dataset. Reviewer Agent monitors execution.

   \* Stage 4: Workflow Results Review: Reviewer Agent validates the final output. It ensures the New Derived SDS metadata contains provenance links (wasDerivedFrom) pointing to the Input SDS and the specific nnU-Net Tool/Workflow used.

Example B: Cardiac Digital Twin (The "biv-me" Workflow)

 \* Source: An Open-Source End-to-End Pipeline for Generating 3D+t Biventricular Meshes.

 \* The Goal (Replication): Create a digital twin of a heart (biventricular mesh) from the raw CMR images provided in the paper's sample repository.

 \* VeriFlow Execution:

   \* Stage 1: Workflow Description Templating: Scholar Agent maps the pipeline (View Classification \\rightarrow Segmentation \\rightarrow Contours \\rightarrow Model Fitting). It creates separate SDS datasets for the Investigation/Study context. It creates Tool SDSs for the pipeline code and Model SDSs for the pre-trained ResNet and U-Net weights used in the classification and segmentation steps. It creates an Input Measurement SDS for the raw CMR images (metadata only).

   \* Stage 2: Workflow Execution Templating: Engineer Agent scans the repo. It generates a "Data Validator" adapter. It ensures that the final biventricular mesh is stored as defined by the SDS Blueprints, linking the execution back to the specific Assay and Study.

   \* Stage 3: Workflow Execution and Diagnostics: During the run (orchestrated by Airflow 3 and iterating through all subjects), the "Model Fitting" step fails on one of the sample subjects. The Reviewer Agent analyzes the logs: "The solver failed to converge for Subject 004."

   \* Stage 4: Workflow Results Review: Once successful, the Reviewer Agent checks the temporal coherence of the output mesh.

4\. User Interface & Experience Strategy

The user interface for VeriFlow is designed to be minimalistic, modular, and highly flexible. To help with development and maintenance, the system is strictly modularised. When designing the UI/UX, only the hackathon examples (Breast Cancer Segmentation and Cardiac Digital Twin) should be used.

General UI Requirements:

 \* Collapsible & Adjustable: All modules are collapsible, and their borders are adjustable.

 \* Global Configuration: There should be general configuration options, for example, to indicate how often the Scholar Agent should re-analyse the paper once the user edits properties of the nodes (e.g., when the user asks for it or presses a "re-analyse" button, or if it should be done each time the user updates a property). The system will keep track of which properties are edited by the user to ensure the Scholar does not overwrite them during re-analysis.

 \* Export: The ability to export each node should be provided at any time, allowing the user to download the SDS associated with that node (e.g., via the catalogue). There should also be an export all feature.

 \* Placement:

   \* As the user may dynamically upload information when interacting with the Reviewer, the Upload Module should be on the same page as the Workflow Assembler Module.

   \* As the user will be dragging items from the catalogue into the workflow assembly, the Catalogue Module will also need to be part of the same page as the Workflow Assembler.

Module Definitions:

(Note: The numbering below does not reflect the order in which the user uses the modules).

 \* Upload Module: For uploading papers (and other associated files).

 \* Catalogue Module: The catalogue should automatically populate as data objects are discovered, and descriptions dynamically updated when edited by the user. The different data objects should be separated by collapsible and scrollable panels. Some of the objects will be able to be dragged and dropped into the study design selector and workflow assembler module in the future.

 \* Property Editor Module: Each time you click on a node in either the Study Design Selector Module or the Workflow Assembler Module, you should be able to edit their properties. The properties available will be dependent on the type of node.

 \* Console Module: Where the user will be able to converse with VeriFlow. Requires time-stamped output descriptions, showing information about what the agents are doing and progress.

 \* Study Design Selector Module: Once a paper has been uploaded, the Scholar Agent should dynamically and automatically show the study design nodes. These include:

   \* Single Paper root node with paper name property.

   \* One or more Investigation nodes: has one editable description property.

   \* One or more Study nodes: has one editable description property.

   \* One or more Assay nodes: has multiple editable numbered properties each describing sequential steps of the workflow. Workflow step descriptions should be able to be edited, removed, added with undo functionality. It should be possible to insert a new step in the middle of existing steps if needed.

   \* Selection: The user should be able to select the assay that they want to assemble the workflow for.

 \* Workflow Assembler Module: Once an assay has been selected, the Scholar Agent should dynamically and automatically populate the workflow diagram.

   \* Structure: The workflow doesn’t have to have a node itself. It would have a flow from left to right with an input measurement node on the left end and an output measurement node on the right end.

   \* Measurement Nodes: Will have multiple properties defining the dataset and sample pairs that will be used as either inputs or outputs. Properties are autopopulated by the Scholar, but the user should be able to add/re-order/insert/remove properties by clicking a plus button. This brings up a dropdown menu to allow the user to select from Scholar-identified values or create new entries (e.g., manually define a new sample). This can also be done by conversing with VeriFlow in the chat, dynamically updating the interface. The Scholar Agent takes these updates into account when re-analysing the paper. This module should also mention the number of subjects available in the selected measurement.

   \* Run Configuration: Separately, in one place, the number of subjects the workflow should run on should also be specified.

   \* Model Nodes: Similar to measurement nodes except they have a single model property that can be linked to, e.g., the input of a tool.

   \* Tool Nodes: Can have multiple inputs and multiple outputs. Each will be autopopulated by the Scholar Agent. Like measurements, the user can add/re-order/insert/remove these inputs or outputs. All possible inputs/outputs appear when the user tries to add or modify via a dropdown or custom text entry, but only those used will be shown. Any unused inputs/outputs do not need to be shown, indicating they take default values (e.g., from documentation).

   \* Execution: Workflow execution can be integrated with this module with a run all button and the ability to re-run individual tools (e.g., if node properties are updated). When executed, the status should be indicated for each tool in the module. The ability to abort should be included.

   \* Diagnostics: Execution UI elements are dynamically updated by querying the Airflow API. If the user wants more detailed info, we provide a link to the Airflow Dashboard UI running as a separate service. The Engineer Agent dynamically generates container files. The Reviewer Agent indicates information about each tool via icons and provides a confidence score on each node.

 \* Dataset Navigation Module: Allows the user to navigate through the SDS directory tree and use plugins to modify or edit files (e.g., SDS metadata files or CWL files). An appropriate view with relevant syntax highlighting should be used.

 \* Plugins Module: Allows custom modules to be added to VeriFlow.

   \* Functionality: Actions can be registered/associated. For example, when a user selects an applicable file in the Dataset Navigation Module, they could right-click/press and hold on a file to select which plugin to use.

   \* Examples: If a text file is selected, an option to open a file editor panel will appear (e.g., within the plugins module). If a 3D segmentation or 3D medical image is selected, then a  VolView panel will appear.

6\. Hackathon Phases (Development Roadmap)

The development during the hackathon is separated into three distinct phases which map to the VeriFlow stages and utilize the modules described above.

Phase 1: Stage 1 \- Workflow Description Templating

 \* Focus: The Scholar Agent analyzes the paper to extract the study design (in ISA), workflows (in CWL), tools (in CWL), measurements, and models, creating an SDS with metadata for them all.

 \* Action: The focus is on identifying the investigation, study, and assays in the paper using the Study Design Selector Module. The system allows the user to select which study and assays they want to replicate (e.g., for the hackathon we choose the study that describes running the workflow (Assay 1\) and evaluating the workflow (Assay 2)). This helps pin down the specific measurements and tools used, and the system can ask the user to confirm these selections.

 \* Subject Limiting: Some of the studies described in the paper might use a large number of subjects, requiring large datasets to be accessed and downloaded. Instead, during the generation of workflow description, the system allows the user to modify the number of subjects to be considered when the workflow is executed to get the data and run the workflow (specified in the Workflow Assembler Module). Editing the number of subjects could be either via chat or by updating the subjects field of the measurements SDS (defined in the SDS dataset description file). As specific subject ids may not be defined in the paper, the system can default to running the first subject.

 \* SDS Population: At this stage, the assay files and their metadata are added to the Assay SDS, while only the measurement metadata is added to the Measurement SDS.

 \* Support: The Reviewer Agent helps with verifying the extraction and supporting the user (e.g., running scientific unit-tests, red teaming the extraction logic). The user can download the SDS datasets generated at this stage.

Phase 2: Stage 2 & 3 \- Workflow Execution Templating & Diagnostics

 \* Stage 2 (Execution Templating): The Engineer Agent creates containers for tools based on the Scholar’s CWL descriptions, sorts out dependencies, and dynamically adds adapters to map inputs between tools. The user interacts with the Workflow Assembler Module and Property Editor Module to fine-tune inputs and outputs.

 \* Stage 3 (Execution and Diagnostics): It handles the execution of the workflow with Airflow. Running the workflow involves running each tool for each subject in the measurement dataset.

 \* Support: The Reviewer Agent helps review the Engineer's blueprint before running. During running, it flags errors and proposes alternatives (including red teaming and Runtime Diagnostics displayed in the Console Module). The user can download the updated SDS datasets (including intermediate results if selected).

Phase 3: Stage 4 \- Workflow Results Review

 \* Focus: The Reviewer Agent helps compare outputs in the paper with outputs generated by the executed workflow. Note that generating the specific outputs that would be compared is already part of the study/assays defined in Phase 1, so the focus here is strictly on comparing the results in the paper.

 \* Extension: One thing that might be needed is to add a new tool (e.g., DICE overlap) if the paper/github doesn’t provide that (this relates to the "extension scenario" future item).

 \* Visualisation: For visualisation of results, for now, we simplify things to let the user navigate to the measurement dataset generated by the executed workflow using the Dataset Navigation Module and use the built-in viewers (e.g., VolView triggered via the Plugins Module) to view the results. The user can download the final SDS datasets containing the replication results.

